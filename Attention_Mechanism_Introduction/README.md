# Understanding the attention mechanism in Vision transformers

This repository implements the multi-headed self attention layer in PyTorch.
